{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_fr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPW1X9IKsV1bHQvkkhozifo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Philippe-AD/Jupyter/blob/master/spacy_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCNprrn_3RCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy-lefff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGTm-F0f6WyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcB2Q8Bw3tFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "nlp = spacy.load('fr')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j79TYhU91RSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "f4e987a1-cb03-4c57-c403-5b81fd62d961"
      },
      "source": [
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer, POSTagger\n",
        "\n",
        "nlp = spacy.load('fr')\n",
        "french_lemmatizer = LefffLemmatizer()\n",
        "nlp.add_pipe(french_lemmatizer, name='lefff')\n",
        "doc = nlp(u\"Apple cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
        "for d in doc:\n",
        "    print(d.text, d.pos_, d._.lefff_lemma, d.tag_, d.lemma_)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-04 07:01:32,686 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.\n",
            "2020-09-04 07:01:32,687 - spacy_lefff.lefff - INFO - Token lefff_lemma already registered\n",
            "2020-09-04 07:01:32,687 - spacy_lefff.lefff - INFO - Reading lefff data...\n",
            "2020-09-04 07:01:33,278 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer\n",
            "Apple PROPN None PROPN___ Apple\n",
            "cherche NOUN cherche NOUN__Gender=Fem|Number=Sing cherche\n",
            "a AUX None AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin avoir\n",
            "acheter VERB acheter VERB__VerbForm=Inf acheter\n",
            "une DET un DET__Definite=Ind|Gender=Fem|Number=Sing|PronType=Art un\n",
            "startup NOUN None NOUN__Gender=Fem|Number=Sing startup\n",
            "anglaise NOUN anglaise NOUN__Gender=Fem|Number=Sing anglaise\n",
            "pour ADP None ADP___ pour\n",
            "1 NUM None NUM__NumType=Card 1\n",
            "milliard NOUN milliard NOUN__Gender=Masc|Number=Sing|NumType=Card milliard\n",
            "de ADP un ADP___ de\n",
            "dollard ADJ None ADJ__Number=Plur dollard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2l4U96-6r_2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# the text to be tokenized \n",
        "text = (\"Natural language processing (NLP) is a field \" +\n",
        "\t\"of computer science, artificial intelligence \" +\n",
        "\t\"and computational linguistics concerned with \" +\n",
        "\t\"the interactions between computers and human \" +\n",
        "\t\"(natural) languages, and, in particular, \" +\n",
        "\t\"concerned with programming computers to \" +\n",
        "\t\"fruitfully process large natural language \" +\n",
        "\t\"corpora. Challenges in natural language \" +\n",
        "\t\"processing frequently involve natural \" +\n",
        "\t\"language understanding, natural language\" +\n",
        "\t\"generation frequently from formal, machine\" +\n",
        "\t\"-readable logical forms), connecting language \" +\n",
        "\t\"and machine perception, managing human-\" +\n",
        "\t\"computer dialog systems, or some combination \" +\n",
        "\t\"thereof.\") \n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQK7oRWE7bxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e5dcdb07-35c6-4a66-8921-980deb11c347"
      },
      "source": [
        "import spacy # tested with v2.2.3\n",
        "from spacy.pipeline import EntityRecognizer\n",
        "\n",
        "text = \"Jane lives in Boston. Jan lives in Bremen.\"\n",
        "\n",
        "# load the English and German models\n",
        "nlp_en = spacy.load('en_core_web_sm')  # NER tags PERSON, GPE, ...\n",
        "nlp_de = spacy.load('fr') # NER tags PER, LOC, ...\n",
        "\n",
        "# the Vocab objects are not the same\n",
        "assert nlp_en.vocab != nlp_de.vocab\n",
        "\n",
        "# but the vectors are identical (because neither model has vectors)\n",
        "assert nlp_en.vocab.vectors.to_bytes() == nlp_de.vocab.vectors.to_bytes()\n",
        "\n",
        "# original English output\n",
        "doc1 = nlp_en(text)\n",
        "print([(ent.text, ent.label_) for ent in doc1.ents])\n",
        "# [('Jane', 'PERSON'), ('Boston', 'GPE'), ('Bremen', 'GPE')]\n",
        "\n",
        "# original German output (the German model makes weird predictions for English text)\n",
        "doc2 = nlp_de(text)\n",
        "print([(ent.text, ent.label_) for ent in doc2.ents])\n",
        "# [('Jane lives', 'PER'), ('Boston', 'LOC'), ('Jan lives', 'PER'), ('Bremen', 'LOC')]\n",
        "\n",
        "# initialize a new NER component with the vocab from the English pipeline\n",
        "ner_de = EntityRecognizer(nlp_en.vocab)\n",
        "\n",
        "# reload the NER component from the German model by serializing\n",
        "# without the vocab and deserializing using the new NER component\n",
        "ner_de.from_bytes(nlp_de.get_pipe(\"ner\").to_bytes(exclude=[\"vocab\"]))\n",
        "\n",
        "# add the German NER component to the end of the English pipeline\n",
        "nlp_en.add_pipe(ner_de, name=\"ner_de\")\n",
        "\n",
        "# check that they have the same vocab\n",
        "assert nlp_en.vocab == ner_de.vocab\n",
        "\n",
        "# combined output (English NER runs first, German second)\n",
        "doc3 = nlp_en(text)\n",
        "print([(ent.text, ent.label_) for ent in doc3.ents])\n",
        "# [('Jane', 'PERSON'), ('Boston', 'GPE'), ('Jan lives', 'PER'), ('Bremen', 'GPE')]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Jane', 'PERSON'), ('Boston', 'GPE'), ('Bremen', 'GPE')]\n",
            "[('Jane lives in Boston', 'MISC')]\n",
            "[('Jane', 'PERSON'), ('Boston', 'GPE'), ('Jan', 'PER'), ('Bremen', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}