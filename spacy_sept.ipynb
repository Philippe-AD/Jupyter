{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_sept.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMi25tHNkm5zeZ704fbMSvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Philippe-AD/Jupyter/blob/master/spacy_sept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvHETsGxyk5E",
        "colab_type": "text"
      },
      "source": [
        "# Rule-Based Matching with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz8hzNRSxllV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63f13bd1-0000-405c-ff16-0eed0aa73761"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"(?i)[0-9]+(?:[,.][0-9]+)?[ckwh]+\"}}, \n",
        "           {'ORTH': '/'}, {\"LOWER\": {\"REGEX\": \"(?i)^[ckwh]+$\"}}]\n",
        "matcher.add(\"Usage\", None, pattern)\n",
        "\n",
        "doc = nlp(\"Peak Usage 409 24.51c/kWh $100.25\")\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17478281289916104085 Usage 3 6 24.51c/kWh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5tNnyiBy4nP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "6d17e1bf-f8aa-4591-bb99-68b9898aa501"
      },
      "source": [
        "#Matcher.py\n",
        "#import necessary modules and tools \n",
        "import spacy\n",
        "from spacy.matcher import Matcher #import Matcher class from spacy\n",
        "#import the Span class to extract the words from the document object\n",
        "from spacy.tokens import Span \n",
        "#Language class with the English model 'en_core_web_sm' is loaded\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#instantiate a new Matcher class object \n",
        "matcher = Matcher(nlp.vocab)\n",
        "#define the pattern\n",
        "pattern = [{'LOWER': 'computer', 'POS': 'NOUN'},\n",
        "             {'POS':{'NOT_IN': ['VERB']}}]\n",
        "#add the pattern to the previously created matcher object\n",
        "matcher.add(\"Matching\", None, pattern)\n",
        "#The input text string is converted to a Document object\n",
        "doc = nlp(\"\"\"Computer programming is the process of writing instructions that get executed by computers. The instructions, also known as code, \n",
        "             are written in a programming language which the computer can understand and use to perform a task or solve a problem. \n",
        "             Basic computer programming involves the analysis of a problem and development of a logical sequence of instructions to solve it. \n",
        "             There can be numerous paths to a solution and the computer programmer seeks to design and code that which is most efficient. \n",
        "             Among the programmer’s tasks are understanding requirements, determining the right programming language to use, designing or architecting the solution, \n",
        "             coding, testing, debugging and writing documentation so that the solution can be easily understood by other programmers.Computer programming is at \n",
        "             the heart of computer science. It is the implementation portion of software development, application development and software engineering efforts, \n",
        "             transforming ideas and theories into actual, working solutions.\"\"\")\n",
        "#call the matcher object the document object and it will return #match_id, start and stop indexes of the matched words\n",
        "matches = matcher(doc)\n",
        "#print the matched results and extract out the results\n",
        "for match_id, start, end in matches:\n",
        "    # Get the string representation \n",
        "    string_id = nlp.vocab.strings[match_id]  \n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6895354335150655416 Matching 0 2 Computer programming\n",
            "6895354335150655416 Matching 47 49 computer programming\n",
            "6895354335150655416 Matching 78 80 computer programmer\n",
            "6895354335150655416 Matching 136 138 Computer programming\n",
            "6895354335150655416 Matching 144 146 computer science\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQnDpGPszhwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "f8a27da4-d469-404a-bb3d-8defcc446aaf"
      },
      "source": [
        "# PhraseMatcher.py\n",
        "# import necessary modules\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher #import PhraseMatcher class\n",
        "# Language class with the English model 'en_core_web_sm' is loaded\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# create the PhraseMatcher object\n",
        "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
        "# the list containing the pharses to be matched\n",
        "terminology_list = [\"Machine Learning\", \"Hidden Structure\",              \n",
        "                           \"Unlabeled Data\"]\n",
        "# convert the phrases into document object using nlp.make_doc to #speed up.\n",
        "patterns = [nlp.make_doc(text) for text in terminology_list]\n",
        "# add the patterns to the matcher object without any callbacks\n",
        "matcher.add(\"Phrase Matching\", None, *patterns)\n",
        "# the input text string is converted to a Document object\n",
        "doc = nlp(\"\"\"Supervised machine learning algorithms can apply what has been learned in the past to new data using labeled examples to predict future events. \n",
        "Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. \n",
        "The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, \n",
        "intended output and find errors in order to modify the model accordingly. In contrast, unsupervised machine learning algorithms are used when the information \n",
        "used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. \n",
        "The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe hidden structures from unlabeled data. \n",
        "Semi-supervised machine learning algorithms fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data \n",
        "for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve \n",
        "learning accuracy. Usually, semi-supervised learning is chosen when the acquired labeled data requires skilled and relevant resources in order to \n",
        "train it / learn from it. Otherwise, acquiring unlabeled data generally doesn’t require additional resources.\"\"\")\n",
        "#call the matcher object the document object and it will return #match_id, start and stop indexes of the matched words\n",
        "matches = matcher(doc)\n",
        "#print the matched results and extract out the results\n",
        "for match_id, start, end in matches:\n",
        "    # Get the string representation \n",
        "    string_id = nlp.vocab.strings[match_id]  \n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11356100181062323261 Phrase Matching 1 3 machine learning\n",
            "11356100181062323261 Phrase Matching 96 98 machine learning\n",
            "11356100181062323261 Phrase Matching 126 128 hidden structure\n",
            "11356100181062323261 Phrase Matching 129 131 unlabeled data\n",
            "11356100181062323261 Phrase Matching 159 161 unlabeled data\n",
            "11356100181062323261 Phrase Matching 166 168 machine learning\n",
            "11356100181062323261 Phrase Matching 184 186 unlabeled data\n",
            "11356100181062323261 Phrase Matching 202 204 unlabeled data\n",
            "11356100181062323261 Phrase Matching 252 254 unlabeled data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnT_trTV0HjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f9353521-bf62-4a2c-cbf8-bb39cde3f693"
      },
      "source": [
        "# EntityRuler.py\n",
        "# import necessary packages and tools \n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler # import EntityRuler \n",
        "# load a blank English model from spacy\n",
        "nlp = spacy.blank('en')\n",
        "# convert the input sentence into the document object \n",
        "doc = nlp(\"The First Estate included the clergy (church leaders), the Second Estate included the nobles, and the Third Estate included the commoners. The Third Estate paid most of the taxes, while the nobility lived lives of luxury and got all the high-ranking jobs.\")\n",
        "# print the entity types of each entity in the above sentence\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# instantiate an object of EntityRuler class\n",
        "ruler = EntityRuler(nlp)\n",
        "# define the pattern\n",
        "patterns = [{\"label\": \"NOUN\", \"pattern\": \"church\"}, \n",
        "             {\"label\": \"ORG\",              \n",
        "             \"pattern\": [{\"lower\": \"the\"}, \n",
        "             {\"lower\": {\"IN\": [\"first\", \"second\", \"third\"]}},                          \n",
        "             {\"ORTH\": \"Estate\"}]}]\n",
        "# add the pattern to the matcher object\n",
        "ruler.add_patterns(patterns)\n",
        "# add the matcher object as a new pipe to the model\n",
        "nlp.add_pipe(ruler)\n",
        "# convert the input sentence into the document object using the newly added 'nlp'\n",
        "doc = nlp(\"The First Estate included the clergy (church leaders), the Second Estate included the nobles, and the Third Estate included the commoners. The Third Estate paid most of the taxes, while the nobility lived lives of luxury and got all the high-ranking jobs.\")\n",
        "# print the entities in the sentenced after adding the EntityRuler matcher\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[('The First Estate', 'ORG'), ('church', 'NOUN'), ('the Second Estate', 'ORG'), ('the Third Estate', 'ORG'), ('The Third Estate', 'ORG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JBcB22w01xj",
        "colab_type": "text"
      },
      "source": [
        "# Spacy Fr\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAqwQzvS081W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "eba6c6ad-c596-4284-8347-be743e17c2ce"
      },
      "source": [
        "from spacy.lang.fr import French\n",
        "\n",
        "nlp = French()\n",
        "\n",
        "# Traite le texte\n",
        "doc = nlp(\n",
        "    \"En 1990, plus de 60 % de la population d'Asie orientale vivait dans une pauvreté extrême. \"\n",
        "    \"Actuellement c'est moins de 4 %.\"\n",
        ")\n",
        "\n",
        "# Itère sur les tokens du doc\n",
        "for token in doc:\n",
        "    print(token)\n",
        "    # Vérifie si le token ressemble à un nombre\n",
        "    if token.like_num:\n",
        "        # Obtiens le token suivant dans le document\n",
        "        next_token = doc[token.i + 1]\n",
        "        # Vérifie si le texte du token suivant est égal à \"%\"\n",
        "        if next_token.text == \"%\":\n",
        "            print(\"Pourcentage trouvé :\", token.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "En\n",
            "1990\n",
            ",\n",
            "plus\n",
            "de\n",
            "60\n",
            "Pourcentage trouvé : 60\n",
            "%\n",
            "de\n",
            "la\n",
            "population\n",
            "d'\n",
            "Asie\n",
            "orientale\n",
            "vivait\n",
            "dans\n",
            "une\n",
            "pauvreté\n",
            "extrême\n",
            ".\n",
            "Actuellement\n",
            "c'\n",
            "est\n",
            "moins\n",
            "de\n",
            "4\n",
            "Pourcentage trouvé : 4\n",
            "%\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0NIBjZ_6fEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Kn6rkp6VD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "67128e87-89fd-4054-820a-cdddd89e7ac0"
      },
      "source": [
        "from spacy.lang.fr import French\n",
        "\n",
        "nlp = French()\n",
        "\n",
        "# Traite le texte\n",
        "doc = nlp(\n",
        "    \"En 1990, plus de 60 % de la population d'Asie orientale vivait dans une pauvreté extrême. \"\n",
        "    \"Actuellement c'est moins de 4 %.\"\n",
        ")\n",
        "\n",
        "# Itère sur les tokens du doc\n",
        "for token in doc:\n",
        "    print(token)\n",
        "    # Vérifie si le token ressemble à un nombre\n",
        "    if token.like_num:\n",
        "        # Obtiens le token suivant dans le document\n",
        "        next_token = doc[token.i + 1]\n",
        "        # Vérifie si le texte du token suivant est égal à \"%\"\n",
        "        if next_token.text == \"%\":\n",
        "            print(\"Pourcentage trouvé :\", token.text)\n",
        "\n",
        "print(\"*\"*80)\n",
        "doc = nlp(\"Importer un modèle LUIS et ajouter des intentions\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos, token.dep)\n",
        "\n",
        "for token in doc: \n",
        "  print(token, token.lemma_)     "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "En\n",
            "1990\n",
            ",\n",
            "plus\n",
            "de\n",
            "60\n",
            "Pourcentage trouvé : 60\n",
            "%\n",
            "de\n",
            "la\n",
            "population\n",
            "d'\n",
            "Asie\n",
            "orientale\n",
            "vivait\n",
            "dans\n",
            "une\n",
            "pauvreté\n",
            "extrême\n",
            ".\n",
            "Actuellement\n",
            "c'\n",
            "est\n",
            "moins\n",
            "de\n",
            "4\n",
            "Pourcentage trouvé : 4\n",
            "%\n",
            ".\n",
            "********************************************************************************\n",
            "Importer 0 0\n",
            "un 0 0\n",
            "modèle 0 0\n",
            "LUIS 0 0\n",
            "et 0 0\n",
            "ajouter 0 0\n",
            "des 0 0\n",
            "intentions 0 0\n",
            "Importer Importer\n",
            "un un\n",
            "modèle modèle\n",
            "LUIS LUIS\n",
            "et et\n",
            "ajouter ajouter\n",
            "des des\n",
            "intentions intentions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQSoJblA8P8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "eab444c5-042f-4eac-86e8-437fbd60776c"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.fr import French\n",
        "\n",
        "nlp = French()\n",
        "\n",
        "doc = nlp(\"Demain je travaille à la maison.\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_,\n",
        "        token.ent_type_\n",
        "    ))\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(\">\", ent.text, ent.label_)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Demain\t0\tDemain\tFalse\tFalse\tXxxxx\t\t\t\n",
            "je\t7\tje\tFalse\tFalse\txx\t\t\t\n",
            "travaille\t10\ttravaille\tFalse\tFalse\txxxx\t\t\t\n",
            "à\t20\tà\tFalse\tFalse\tx\t\t\t\n",
            "la\t22\tla\tFalse\tFalse\txx\t\t\t\n",
            "maison\t25\tmaison\tFalse\tFalse\txxxx\t\t\t\n",
            ".\t31\t.\tTrue\tFalse\t.\t\t\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJ2EK7vHtpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(\"I live in Guwahati, Assam\")\n",
        "displacy.render(doc, style=\"dep\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5owmGBwEyGSS",
        "colab_type": "text"
      },
      "source": [
        "Natural language input\n",
        "\n",
        "FirstSeed Calendar has a powerful natural-language processing engine that lets you almost \"speak\" to the app to enter a new event or reminder. So you can type **\"Dinner at 7 pm tonight\"** or **\"Remind me to call John at noon\"**. Or better yet, you can use Siri's voice dictation to literally create new events or reminders simply by speaking.\n",
        "\n",
        "Natural language input is currently available in English, German and Japanese. More languages will be supported in the future versions. "
      ]
    }
  ]
}